# 📘 **Curso Python e SageMaker: De A-Z**
Bem-vindo ao repositório do curso **Python e SageMaker**! Este repositório é uma coleção de projetos que abrangem técnicas fundamentais e avançadas de Machine Learning e análise de dados, organizados em duas seções principais: **Python** e **SageMaker**.

---

## 🔗 **Sumário**
1. [Curso de Python de A-Z](#python-de-a-z)  
   1.1 [Regressão Linear](#regressão-linear)  
   1.2 [Algoritmo Apriori](#algoritmo-apriori)  
   1.3 [K-Means e Agrupamentos](#k-means-e-agrupamentos)  
   1.4 [Redução de Dimensionalidade](#redução-de-dimensionalidade)  
   1.5 [Detecção de Outliers](#detecção-de-outliers)  
2. [Curso de SageMaker](#curso-de-sagemaker)  
   2.1 [Bike Random Cut](#bike-random-cut)  
   2.2 [Bikes DeepAR](#bikes-deepar)  
   2.3 [Regressão Linear - Censo](#regressão-linear-censo)  
   2.4 [Regressão Linear - Casas](#regressão-linear-casas)  
   2.5 [PCA e Agrupamento](#pca-e-agrupamento)  
   2.6 [XGBoost](#xgboost)  

---

## 🐍 **Python de A-Z**

### 📊 **1. Regressão Linear ([RL.ipynb](RL.ipynb))**
**Objetivo:** Estudar e implementar a regressão linear para prever variáveis contínuas.  
**Principais aprendizados:**  
- Conceito de regressão linear simples e múltipla.  
- Uso do `scikit-learn` para ajuste de modelos.  
- Métricas como R² e MSE para avaliação de performance.  
- Importância da visualização na interpretação de relações.

---

### 🛒 **2. Algoritmo Apriori ([Apriori.ipynb](Apriori.ipynb))**  
**Objetivo:** Identificar padrões frequentes em bases de dados transacionais.  
**Principais aprendizados:**  
- Aplicação do algoritmo Apriori para regras de associação.  
- Métricas como suporte, confiança e lift.  
- Preparação e limpeza de dados para mineração eficiente.

---

### 📈 **3. K-Means e Agrupamentos ([kmeans_e_agrupamentos.ipynb](kmeans_e_agrupamentos.ipynb))**  
**Objetivo:** Implementar agrupamento não supervisionado usando K-Means.  
**Principais aprendizados:**  
- Método do cotovelo para definir o número ideal de clusters.  
- Visualização 2D para análise de separação dos grupos.  
- Impacto da inicialização aleatória nos resultados.

---

### 📉 **4. Redução de Dimensionalidade ([reducaodedados.ipynb](reducaodedados.ipynb))**  
**Objetivo:** Reduzir variáveis mantendo informações relevantes com PCA.  
**Principais aprendizados:**  
- Avaliação de variância explicada para validar a redução.  
- Benefícios em processamento e visualização.

---

### ⚠ **5. Detecção de Outliers ([outliers.ipynb](outliers.ipynb))**  
**Objetivo:** Identificar e tratar outliers para dados mais confiáveis.  
**Principais aprendizados:**  
- Métodos estatísticos como IQR e z-score.  
- Visualização com boxplots e histogramas.  
- Decisões estratégicas para remover ou ajustar outliers.

---

## ☁️ **Curso de SageMaker**

### 🛵 **1. Bike Random Cut ([bike-random-cut.ipynb](bike-random-cut.ipynb))**  
**Objetivo:** Prever padrões relacionados a bicicletas usando particionamento aleatório.  
**Principais aprendizados:**  
- Exploração de dados sazonais e temporais.  
- Importância da análise exploratória.  

---

### 🔮 **2. Bikes DeepAR ([bikes-deepar.ipynb](bikes-deepar.ipynb))**  
**Objetivo:** Prever séries temporais com o modelo DeepAR.  
**Principais aprendizados:**  
- Configuração e treinamento do DeepAR no SageMaker.  
- Métricas como RMSE e MAE para validação.

---

### 📊 **3. Regressão Linear - Censo ([linear-leaner-census.ipynb](linear-leaner-census.ipynb))**  
**Objetivo:** Previsão socioeconômica com regressão linear.  
**Principais aprendizados:**  
- Interpretação de coeficientes no contexto do censo.

---

### 🏡 **4. Regressão Linear - Casas ([linear-leaner-house.ipynb](linear-leaner-house.ipynb))**  
**Objetivo:** Prever preços de casas com variáveis categóricas.  
**Principais aprendizados:**  
- Codificação one-hot e impacto de variáveis no preço.  

---

### 🔄 **5. PCA e Agrupamento ([pca-agrupamento.ipynb](pca-agrupamento.ipynb))**  
**Objetivo:** Reduzir dimensões e agrupar dados similares.  
**Principais aprendizados:**  
- PCA para dimensionalidade e K-Means para agrupamentos.

---

### 🏆 **6. XGBoost ([xgboost.ipynb](xgboost.ipynb))**  
**Objetivo:** Modelos avançados para classificação e regressão.  
**Principais aprendizados:**  
- Tunagem de hiperparâmetros e validação cruzada.  
- SHAP para avaliação do impacto de features.



